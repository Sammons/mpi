This project uses autotools.

recommended command sequence to get started:

	autoreconf -ivf
	./configure
	make

a binary will be produced in the src directory named  "mpi"

basic usage of this binary

	mpirun src/mpi <data dir> <seed for search vector> <number of nearest neighbors>

for example

	mpirun src/mpi /cluster/content/hpc/dev_data/ 3 2000

-----------

My goal in this lab was to spend as little time
in the queues as possible while still distributing the work 
relatively evenly.

I had problems with my old algorithm, where it would let duplicate
image_ids in the partial results shove out low-ranking images. so
when there weren't enough unique image ids the nearest neighbors
would falsely return 0's in place of some that should not have been.
- in this iteration those problems are fixed, but at the cost of some efficiency

----------
Algorithm:

let N be the number of nodes

1) generate single vector to search against

2) read data directory recursively to get all data files

3) divide up data file names into N groups.

4) send every node a collection of file names

5) receive the file names, and for each file represented

    5.1) read every entry into a vector structure, 
         only tracking image_id, and the 128 element vector

    5.2) rank every vector against the search vector,
         using manhattan distance, providing a collection of rankings

    5.3) associate every ranking with its image_id in a map, deduplicating the image_ids.
         for any image_id, the associated distance is the smallest distance found for
         that image.

    5.4) convert the map back into a vector of ranks, and sort by distance

    5.3) serialize ALL of the collection of rankings into a new partial result file. 
    	 this serialization is literally just writing down the binary.
          
          Note: this is significant, because this dislocates the algorithm in such a way
                that it is not more efficient when fewer / more nearest neighbors are 
                specified. no matter what, it always serializes the sorted ranks for
                every vector in the data file. This allows the query to be totally correct
                in comparison to previous homeworks, where my algorithm could be wrong
                given a small pool of images.

    5.4) return the name of the file saved

6) aggregate the new file names, and send them back to the master node to indicate completion

7) all children are completed, now only the master has work to do

8) aggregate all of the new file names

9) for each of these new files, read the rankings into a "global" map. global in that all of the rankings
    from all of the partial results will end up in this map. again, this acts as a deduplication, where
    the image_id will only be associated with the shortest distance available. it also decreases the 
    "weight" of the computation in the long run, since more unique images will have to be included
    for this set to become significantly large. for comparison the 830k file only has a little over
    1,000 unique image_ids. for that data set this map will only have that same amount of entries.

10) convert the elements of the map back into a vector, and sort it.

11) truncate the vector to the number of nearest neighbors requested.

12) report